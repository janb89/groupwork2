{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import figure\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score ,classification_report, mean_squared_error, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import GridSearchCV, HalvingGridSearchCV\n",
    "\n",
    "# Import models\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier, SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "from tqdm.notebook import tqdm,trange\n",
    "import time \n",
    "\n",
    "# Import scripts\n",
    "from optional.feature_engineering import *\n",
    "from optional.prepare_flight_data import *\n",
    "from optional.dummies import *\n",
    "from optional.predict import *\n",
    "\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DATOP</th>\n",
       "      <th>FLTID</th>\n",
       "      <th>DEPSTN</th>\n",
       "      <th>ARRSTN</th>\n",
       "      <th>STD</th>\n",
       "      <th>STA</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>AC</th>\n",
       "      <th>target</th>\n",
       "      <th>...</th>\n",
       "      <th>delayed</th>\n",
       "      <th>domestic</th>\n",
       "      <th>dep_hour</th>\n",
       "      <th>dep_weekday</th>\n",
       "      <th>duration_min</th>\n",
       "      <th>arr_hour</th>\n",
       "      <th>flight_month</th>\n",
       "      <th>flight_month_name</th>\n",
       "      <th>year</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_id_15674</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>TU 0564</td>\n",
       "      <td>NKC</td>\n",
       "      <td>TUN</td>\n",
       "      <td>2016-01-01 00:15:00</td>\n",
       "      <td>2016-01-01 04:30:00</td>\n",
       "      <td>ATA</td>\n",
       "      <td>TU 320IMV</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Friday</td>\n",
       "      <td>255.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>2016</td>\n",
       "      <td>3298.067996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_id_15676</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>TU 0714</td>\n",
       "      <td>JED</td>\n",
       "      <td>TUN</td>\n",
       "      <td>2016-01-01 00:55:00</td>\n",
       "      <td>2016-01-01 05:30:00</td>\n",
       "      <td>ATA</td>\n",
       "      <td>TU 332IFM</td>\n",
       "      <td>195.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Friday</td>\n",
       "      <td>275.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>2016</td>\n",
       "      <td>3256.052105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_id_15675</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>TU 0614</td>\n",
       "      <td>DKR</td>\n",
       "      <td>TUN</td>\n",
       "      <td>2016-01-01 01:20:00</td>\n",
       "      <td>2016-01-01 05:55:00</td>\n",
       "      <td>ATA</td>\n",
       "      <td>TU 320IMU</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Friday</td>\n",
       "      <td>275.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>2016</td>\n",
       "      <td>3678.974557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_id_30980</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>UG 0002</td>\n",
       "      <td>TUN</td>\n",
       "      <td>DJE</td>\n",
       "      <td>2016-01-01 06:15:00</td>\n",
       "      <td>2016-01-01 07:15:00</td>\n",
       "      <td>SCH</td>\n",
       "      <td>UG AT7LBD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Friday</td>\n",
       "      <td>60.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>2016</td>\n",
       "      <td>333.916459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_id_7179</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>TU 0880</td>\n",
       "      <td>TUN</td>\n",
       "      <td>AMS</td>\n",
       "      <td>2016-01-01 06:30:00</td>\n",
       "      <td>2016-01-01 09:20:00</td>\n",
       "      <td>ATA</td>\n",
       "      <td>TU 736IOP</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Friday</td>\n",
       "      <td>170.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>2016</td>\n",
       "      <td>1770.371959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID                DATOP     FLTID DEPSTN ARRSTN  \\\n",
       "0  train_id_15674  2016-01-01 00:00:00  TU 0564     NKC    TUN   \n",
       "1  train_id_15676  2016-01-01 00:00:00  TU 0714     JED    TUN   \n",
       "2  train_id_15675  2016-01-01 00:00:00  TU 0614     DKR    TUN   \n",
       "3  train_id_30980  2016-01-01 00:00:00  UG 0002     TUN    DJE   \n",
       "4   train_id_7179  2016-01-01 00:00:00  TU 0880     TUN    AMS   \n",
       "\n",
       "                   STD                  STA STATUS         AC  target  ...  \\\n",
       "0  2016-01-01 00:15:00  2016-01-01 04:30:00    ATA  TU 320IMV     0.0  ...   \n",
       "1  2016-01-01 00:55:00  2016-01-01 05:30:00    ATA  TU 332IFM   195.0  ...   \n",
       "2  2016-01-01 01:20:00  2016-01-01 05:55:00    ATA  TU 320IMU    49.0  ...   \n",
       "3  2016-01-01 06:15:00  2016-01-01 07:15:00    SCH  UG AT7LBD     0.0  ...   \n",
       "4  2016-01-01 06:30:00  2016-01-01 09:20:00    ATA  TU 736IOP    36.0  ...   \n",
       "\n",
       "  delayed domestic dep_hour dep_weekday duration_min arr_hour  flight_month  \\\n",
       "0       0        0        0      Friday        255.0        4             1   \n",
       "1       1        0        0      Friday        275.0        5             1   \n",
       "2       1        0        1      Friday        275.0        5             1   \n",
       "3       0        1        6      Friday         60.0        7             1   \n",
       "4       1        0        6      Friday        170.0        9             1   \n",
       "\n",
       "   flight_month_name  year     distance  \n",
       "0            January  2016  3298.067996  \n",
       "1            January  2016  3256.052105  \n",
       "2            January  2016  3678.974557  \n",
       "3            January  2016   333.916459  \n",
       "4            January  2016  1770.371959  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/final_train.csv', index_col=[0])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Firstly we drop a number of columns that we should not use for modeling. Afterwards we separate the data set into features and the response that we are going to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'DATOP', 'FLTID', 'DEPSTN', 'ARRSTN', 'STD', 'STA', 'STATUS',\n",
       "       'AC', 'target', 'icao_DEP', 'iata_DEP', 'name_DEP', 'city_DEP',\n",
       "       'subd_DEP', 'country_DEP', 'elevation_DEP', 'lat_DEP', 'lon_DEP',\n",
       "       'tz_DEP', 'icao_ARR', 'iata_ARR', 'name_ARR', 'city_ARR', 'subd_ARR',\n",
       "       'country_ARR', 'elevation_ARR', 'lat_ARR', 'lon_ARR', 'tz_ARR',\n",
       "       'delay_or_onTime', 'delayed', 'domestic', 'dep_hour', 'dep_weekday',\n",
       "       'duration_min', 'arr_hour', 'flight_month', 'flight_month_name', 'year',\n",
       "       'distance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dropping feature for regression\n",
    "drop_features_reg = ['ID', 'DATOP', 'FLTID', 'AC', 'STATUS', 'DEPSTN', 'ARRSTN', 'flight_month_name', 'delayed', 'year','delay_or_onTime','city_ARR','country_ARR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 102310 entries, 0 to 107832\n",
      "Data columns (total 41 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   ID                 102310 non-null  object \n",
      " 1   DATOP              102310 non-null  object \n",
      " 2   FLTID              102310 non-null  object \n",
      " 3   DEPSTN             102310 non-null  object \n",
      " 4   ARRSTN             102310 non-null  object \n",
      " 5   STD                102310 non-null  object \n",
      " 6   STA                102310 non-null  object \n",
      " 7   STATUS             102310 non-null  object \n",
      " 8   AC                 102310 non-null  object \n",
      " 9   target             102310 non-null  float64\n",
      " 10  icao_DEP           102310 non-null  object \n",
      " 11  iata_DEP           102310 non-null  object \n",
      " 12  name_DEP           102310 non-null  object \n",
      " 13  city_DEP           102310 non-null  object \n",
      " 14  subd_DEP           100266 non-null  object \n",
      " 15  country_DEP        102310 non-null  object \n",
      " 16  elevation_DEP      102310 non-null  float64\n",
      " 17  lat_DEP            102310 non-null  float64\n",
      " 18  lon_DEP            102310 non-null  float64\n",
      " 19  tz_DEP             102310 non-null  object \n",
      " 20  icao_ARR           102310 non-null  object \n",
      " 21  iata_ARR           102310 non-null  object \n",
      " 22  name_ARR           102310 non-null  object \n",
      " 23  city_ARR           102310 non-null  object \n",
      " 24  subd_ARR           100267 non-null  object \n",
      " 25  country_ARR        102310 non-null  object \n",
      " 26  elevation_ARR      102310 non-null  float64\n",
      " 27  lat_ARR            102310 non-null  float64\n",
      " 28  lon_ARR            102310 non-null  float64\n",
      " 29  tz_ARR             102310 non-null  object \n",
      " 30  delay_or_onTime    102310 non-null  object \n",
      " 31  delayed            102310 non-null  int64  \n",
      " 32  domestic           102310 non-null  int64  \n",
      " 33  dep_hour           102310 non-null  int64  \n",
      " 34  dep_weekday        102310 non-null  object \n",
      " 35  duration_min       102310 non-null  float64\n",
      " 36  arr_hour           102310 non-null  int64  \n",
      " 37  flight_month       102310 non-null  int64  \n",
      " 38  flight_month_name  102310 non-null  object \n",
      " 39  year               102310 non-null  int64  \n",
      " 40  distance           102310 non-null  float64\n",
      "dtypes: float64(9), int64(6), object(26)\n",
      "memory usage: 32.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'DATOP', 'FLTID', 'DEPSTN', 'ARRSTN', 'STD', 'STA', 'STATUS',\n",
       "       'AC', 'target', 'icao_DEP', 'iata_DEP', 'name_DEP', 'city_DEP',\n",
       "       'subd_DEP', 'country_DEP', 'elevation_DEP', 'lat_DEP', 'lon_DEP',\n",
       "       'tz_DEP', 'icao_ARR', 'iata_ARR', 'name_ARR', 'city_ARR', 'subd_ARR',\n",
       "       'country_ARR', 'elevation_ARR', 'lat_ARR', 'lon_ARR', 'tz_ARR',\n",
       "       'delay_or_onTime', 'delayed', 'domestic', 'dep_hour', 'dep_weekday',\n",
       "       'duration_min', 'arr_hour', 'flight_month', 'flight_month_name', 'year',\n",
       "       'distance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dropping feature for classification\n",
    "drop_features_class = ['DATOP', 'ID','FLTID','STD', 'STA','target','icao_DEP', 'iata_DEP', 'name_DEP', 'city_DEP',\n",
    "       'subd_DEP', 'country_DEP', 'elevation_DEP', 'lat_DEP', 'lon_DEP',\n",
    "       'tz_DEP', 'icao_ARR', 'iata_ARR', 'name_ARR', 'city_ARR', 'subd_ARR',\n",
    "       'country_ARR', 'elevation_ARR', 'lat_ARR', 'lon_ARR', 'tz_ARR','arr_hour', 'flight_month','delay_or_onTime', 'delayed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'DATOP',\n",
       " 'FLTID',\n",
       " 'AC',\n",
       " 'STATUS',\n",
       " 'DEPSTN',\n",
       " 'ARRSTN',\n",
       " 'flight_month_name',\n",
       " 'delayed',\n",
       " 'year',\n",
       " 'delay_or_onTime',\n",
       " 'city_ARR',\n",
       " 'country_ARR']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_features_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list for categorical predictors/features \n",
    "# (dates are also objects so if you have them in your data you would deal with them first)\n",
    "cat_features = [ 'icao_DEP', 'iata_DEP', 'name_DEP', \n",
    "       'subd_DEP', 'country_DEP',\n",
    "       'tz_DEP' ,'tz_ARR']\n",
    "\n",
    "num_features = ['elevation_ARR', 'distance', 'elevation_DEP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_reg = StandardScaler()\n",
    "scaler_reg.fit(train_df[num_features])\n",
    "train_df[num_features] = scaler_reg.transform(train_df[num_features])\n",
    "#X_test_reg[num_features] = scaler_reg.transform(X_test_reg[num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col2 = OneHotEncoder_labels(train_df, cat_features)\n",
    "\n",
    "#encoder_reg = OneHotEncoder(handle_unknown='ignore', sparse=False, drop='first')\n",
    "#encoder_reg.fit(train_df[cat_features])\n",
    "\n",
    "#X_train_dummie_columns = pd.DataFrame(encoder_reg.transform(train_df[cat_features]))\n",
    "#X_train_reg = train_df.drop(cat_features, axis=1)\n",
    "#X_train_reg = train_df.join(X_train_dummie_columns)\n",
    "#X_train_reg.columns = col2\n",
    "\n",
    "#X_test_dummie_columns = pd.DataFrame(encoder_reg.transform(train_df[cat_features]))\n",
    "#X_test_reg = X_test_reg.drop(cat_features, axis=1)\n",
    "#X_test_reg = X_test_reg.join(X_test_dummie_columns)\n",
    "#X_test_reg.columns = col2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop('target', axis=1)\n",
    "y = train_df.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(labels=drop_features_reg, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['STD', 'STA', 'icao_DEP', 'iata_DEP', 'name_DEP', 'city_DEP',\n",
       "       'subd_DEP', 'country_DEP', 'elevation_DEP', 'lat_DEP', 'lon_DEP',\n",
       "       'tz_DEP', 'icao_ARR', 'iata_ARR', 'name_ARR', 'subd_ARR',\n",
       "       'elevation_ARR', 'lat_ARR', 'lon_ARR', 'tz_ARR', 'domestic', 'dep_hour',\n",
       "       'dep_weekday', 'duration_min', 'arr_hour', 'flight_month', 'distance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(train_df, columns=cat_features, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "# Training data for regression     81848\n",
      "==================\n",
      "Test data\n",
      "# Test data for regression:     20462\n"
     ]
    }
   ],
   "source": [
    "# Split the 'features' and 'target' data into training and testing sets for classification\n",
    "#X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class, y_class, test_size=0.2, stratify=y_class, random_state=RSEED)\n",
    "# Split the 'features' and 'target' data into training and testing sets for classification\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y, test_size=0.2, random_state=RSEED)\n",
    "\n",
    "\n",
    "\n",
    "# Print shape of the test and train data\n",
    "print('Train data')\n",
    "#print('# Training data for classification:     {}'.format(X_train_class.shape[0]))\n",
    "print('# Training data for regression     {}'.format(X_train_reg.shape[0]))\n",
    "print('==================')\n",
    "print('Test data')\n",
    "#print('# Test data for classification:     {}'.format(X_test_class.shape[0]))\n",
    "print('# Test data for regression:     {}'.format(X_test_reg.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['STA'] = X['STA'].str.replace('.', ':', regex=False)\n",
    "X['STA'] = pd.to_datetime(X['STA']).map(pd.Timestamp.timestamp)\n",
    "X['STD'] = pd.to_datetime(X['STD']).map(pd.Timestamp.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['ID'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/janbohlmann/Documents/neuefische/Data_Science/groupwork2/JB_Models.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/janbohlmann/Documents/neuefische/Data_Science/groupwork2/JB_Models.ipynb#ch0000044?line=0'>1</a>\u001b[0m X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39mID\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/neuefische/Data_Science/groupwork2/.venv/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/neuefische/Data_Science/groupwork2/.venv/lib/python3.9/site-packages/pandas/core/frame.py:4906\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4774\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m   4775\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[1;32m   4776\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4783\u001b[0m     errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   4784\u001b[0m ):\n\u001b[1;32m   4785\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4786\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   4787\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4904\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   4905\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4906\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[1;32m   4907\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   4908\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   4909\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   4910\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   4911\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   4912\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[1;32m   4913\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   4914\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/neuefische/Data_Science/groupwork2/.venv/lib/python3.9/site-packages/pandas/core/generic.py:4150\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4148\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   4149\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 4150\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4152\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[1;32m   4153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/Documents/neuefische/Data_Science/groupwork2/.venv/lib/python3.9/site-packages/pandas/core/generic.py:4185\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   4183\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m   4184\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4185\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4186\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreindex(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{axis_name: new_axis})\n\u001b[1;32m   4188\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4189\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/neuefische/Data_Science/groupwork2/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:6017\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6015\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[1;32m   6016\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 6017\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabels[mask]\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6018\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[1;32m   6019\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['ID'] not found in axis\""
     ]
    }
   ],
   "source": [
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 102310 entries, 0 to 107832\n",
      "Columns: 701 entries, ID to tz_ARR_Europe/Zurich\n",
      "dtypes: float64(11), int64(6), object(17), uint8(667)\n",
      "memory usage: 96.4+ MB\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'train_id_15674'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/janbohlmann/Documents/neuefische/Data_Science/groupwork2/JB_Models.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/janbohlmann/Documents/neuefische/Data_Science/groupwork2/JB_Models.ipynb#ch0000040?line=1'>2</a>\u001b[0m X\u001b[39m.\u001b[39minfo()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/janbohlmann/Documents/neuefische/Data_Science/groupwork2/JB_Models.ipynb#ch0000040?line=3'>4</a>\u001b[0m lin_reg2 \u001b[39m=\u001b[39m LinearRegression()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/janbohlmann/Documents/neuefische/Data_Science/groupwork2/JB_Models.ipynb#ch0000040?line=4'>5</a>\u001b[0m lin_reg2\u001b[39m.\u001b[39;49mfit(X, y)\n",
      "File \u001b[0;32m~/Documents/neuefische/Data_Science/groupwork2/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:662\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    658\u001b[0m n_jobs_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs\n\u001b[1;32m    660\u001b[0m accept_sparse \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive \u001b[39melse\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoo\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 662\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    663\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49maccept_sparse, y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    664\u001b[0m )\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    667\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/Documents/neuefische/Data_Science/groupwork2/.venv/lib/python3.9/site-packages/sklearn/base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    580\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    582\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Documents/neuefische/Data_Science/groupwork2/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 964\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    965\u001b[0m     X,\n\u001b[1;32m    966\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m    967\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m    968\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    969\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m    970\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    971\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m    972\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m    973\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m    974\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m    975\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m    976\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    977\u001b[0m )\n\u001b[1;32m    979\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric)\n\u001b[1;32m    981\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/Documents/neuefische/Data_Science/groupwork2/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    745\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 746\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    747\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    748\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    749\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    750\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/neuefische/Data_Science/groupwork2/.venv/lib/python3.9/site-packages/pandas/core/generic.py:1993\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1992\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: NpDtype \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m-> 1993\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'train_id_15674'"
     ]
    }
   ],
   "source": [
    "# Fit linear regression model\n",
    "print(X.info()\n",
    "\n",
    "lin_reg2 = LinearRegression()\n",
    "lin_reg2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate r-squared \n",
    "y_hat2 = lin_reg2.predict(X2)\n",
    "print(\"R-squared:\", r2_score(y2, y_hat2).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"                 Linear Regression Model:\")\n",
    "print(\"===\"*20)\n",
    "print(\"                 RSME:\",mean_squared_error(y_test_reg, y_pred_dumm_reg,squared=False))\n",
    "print(\"===\"*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1abf76b6083f4eb11ada7026e57fce3010dc6b905adaed84ce345c5a9c5e201"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
